{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4d6a06c-db25-412f-aea1-608831ddad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"output.csv\")\n",
    "\n",
    "df['Keywords'] = df['Keywords'].str.split(',')\n",
    "df['GroundTruth Keywords']=df['GroundTruth Keywords'].str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ffc803d-dcf0-4806-9141-9597b6058b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Paraphrased Post</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Take Aways</th>\n",
       "      <th>Highlights</th>\n",
       "      <th>GroundTruth Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.linkedin.com/posts/sahibpreetsingh...</td>\n",
       "      <td>Hello LLM Enthusiasts,\\nStarting a New Day wit...</td>\n",
       "      <td>[Semantic Chunking, Retrieval, RAG]</td>\n",
       "      <td>Chunking involves dividing a document into sma...</td>\n",
       "      <td>How can we break down intricate user inquiries...</td>\n",
       "      <td>[Semantic Chunking, Retrieval, RAG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.linkedin.com/posts/sahibpreetsingh...</td>\n",
       "      <td>Hello, ùóüùóüùó† Enthusiasts, Get Ready for an Adven...</td>\n",
       "      <td>[LLM Enthusiasts, Judge, LLM-generated answers]</td>\n",
       "      <td>LLMs serving as judges,Utilizing N answers fro...</td>\n",
       "      <td>LLMs ascending to the role of a Judge,Challeng...</td>\n",
       "      <td>[LLMs, Judge, Evaluation challenges]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.linkedin.com/posts/sahibpreetsingh...</td>\n",
       "      <td>ùóüùóîùó†ùóî is mastering üêë beneath a ...</td>\n",
       "      <td>[RAG application, RAPTOR, segmenting content]</td>\n",
       "      <td>Consider experimenting with algorithms like TS...</td>\n",
       "      <td>Developing a RAG application,Stumbled upon RAP...</td>\n",
       "      <td>[RAG application, RAPTOR, segmenting content]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.linkedin.com/posts/jobanpreet-sing...</td>\n",
       "      <td>üé§üîé Unveiling the Essentials of...</td>\n",
       "      <td>[Whisper, Wav2Vec2.0, Log-mel Spectrogram]</td>\n",
       "      <td>Whisper has been trained on 680,000 hours of a...</td>\n",
       "      <td>Whisper model trained on refined annotated dat...</td>\n",
       "      <td>[Whisper, Wav2Vec2.0, Log-mel Spectrogram]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.linkedin.com/posts/jobanpreet-sing...</td>\n",
       "      <td>Successfully Finished My Proje...</td>\n",
       "      <td>[English to Punjabi, encoder-decoder framework...</td>\n",
       "      <td>Successfully Finished My Project on Converting...</td>\n",
       "      <td>Sophisticated encoder-decoder framework with d...</td>\n",
       "      <td>[English to Punjabi, encoder-decoder framework...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Link  \\\n",
       "0  https://www.linkedin.com/posts/sahibpreetsingh...   \n",
       "1  https://www.linkedin.com/posts/sahibpreetsingh...   \n",
       "2  https://www.linkedin.com/posts/sahibpreetsingh...   \n",
       "3  https://www.linkedin.com/posts/jobanpreet-sing...   \n",
       "4  https://www.linkedin.com/posts/jobanpreet-sing...   \n",
       "\n",
       "                                    Paraphrased Post  \\\n",
       "0  Hello LLM Enthusiasts,\\nStarting a New Day wit...   \n",
       "1  Hello, ùóüùóüùó† Enthusiasts, Get Ready for an Adven...   \n",
       "2                  ùóüùóîùó†ùóî is mastering üêë beneath a ...   \n",
       "3                  üé§üîé Unveiling the Essentials of...   \n",
       "4                  Successfully Finished My Proje...   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0                [Semantic Chunking, Retrieval, RAG]   \n",
       "1    [LLM Enthusiasts, Judge, LLM-generated answers]   \n",
       "2      [RAG application, RAPTOR, segmenting content]   \n",
       "3         [Whisper, Wav2Vec2.0, Log-mel Spectrogram]   \n",
       "4  [English to Punjabi, encoder-decoder framework...   \n",
       "\n",
       "                                          Take Aways  \\\n",
       "0  Chunking involves dividing a document into sma...   \n",
       "1  LLMs serving as judges,Utilizing N answers fro...   \n",
       "2  Consider experimenting with algorithms like TS...   \n",
       "3  Whisper has been trained on 680,000 hours of a...   \n",
       "4  Successfully Finished My Project on Converting...   \n",
       "\n",
       "                                          Highlights  \\\n",
       "0  How can we break down intricate user inquiries...   \n",
       "1  LLMs ascending to the role of a Judge,Challeng...   \n",
       "2  Developing a RAG application,Stumbled upon RAP...   \n",
       "3  Whisper model trained on refined annotated dat...   \n",
       "4  Sophisticated encoder-decoder framework with d...   \n",
       "\n",
       "                                GroundTruth Keywords  \n",
       "0                [Semantic Chunking, Retrieval, RAG]  \n",
       "1               [LLMs, Judge, Evaluation challenges]  \n",
       "2      [RAG application, RAPTOR, segmenting content]  \n",
       "3         [Whisper, Wav2Vec2.0, Log-mel Spectrogram]  \n",
       "4  [English to Punjabi, encoder-decoder framework...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e70aed4-6ddc-4215-8b18-8beeda3e705c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 6)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "59161a41-cbbc-4074-992b-623824162b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_similarity(list1, list2):\n",
    "    # Lowercase all keywords\n",
    "    list1_lower = [keyword.lower() for keyword in list1]\n",
    "    list2_lower = [keyword.lower() for keyword in list2]\n",
    "    \n",
    "    # Convert lists to sets\n",
    "    set1 = set(list1_lower)\n",
    "    set2 = set(list2_lower)\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    \n",
    "    # Calculate Jaccard similarity\n",
    "    if len(union) == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    jaccard_similarity = len(intersection) / len(union)\n",
    "    \n",
    "    return jaccard_similarity\n",
    "similarities = []\n",
    "for index, row in df.iterrows():\n",
    "    jaccard_sim = calculate_jaccard_similarity(row['Keywords'], row['GroundTruth Keywords'])\n",
    "    similarities.append(jaccard_sim)\n",
    "\n",
    "# Add similarity scores to dataframe\n",
    "df['jaccard_similarity'] = similarities\n",
    "\n",
    "\n",
    "average_similarity = df['jaccard_similarity'].mean()\n",
    "df.loc['Average_jaccard_similarity'] = pd.Series({'jaccard_similarity': average_similarity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2956eede-bac4-422b-9852-ba13a093061b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Paraphrased Post</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Take Aways</th>\n",
       "      <th>Highlights</th>\n",
       "      <th>GroundTruth Keywords</th>\n",
       "      <th>jaccard_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.linkedin.com/posts/sahibpreetsingh...</td>\n",
       "      <td>Hello LLM Enthusiasts,\\nStarting a New Day wit...</td>\n",
       "      <td>[Semantic Chunking, Retrieval, RAG]</td>\n",
       "      <td>Chunking involves dividing a document into sma...</td>\n",
       "      <td>How can we break down intricate user inquiries...</td>\n",
       "      <td>[Semantic Chunking, Retrieval, RAG]</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.linkedin.com/posts/sahibpreetsingh...</td>\n",
       "      <td>Hello, ùóüùóüùó† Enthusiasts, Get Ready for an Adven...</td>\n",
       "      <td>[LLM Enthusiasts, Judge, LLM-generated answers]</td>\n",
       "      <td>LLMs serving as judges,Utilizing N answers fro...</td>\n",
       "      <td>LLMs ascending to the role of a Judge,Challeng...</td>\n",
       "      <td>[LLMs, Judge, Evaluation challenges]</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.linkedin.com/posts/sahibpreetsingh...</td>\n",
       "      <td>ùóüùóîùó†ùóî is mastering üêë beneath a ...</td>\n",
       "      <td>[RAG application, RAPTOR, segmenting content]</td>\n",
       "      <td>Consider experimenting with algorithms like TS...</td>\n",
       "      <td>Developing a RAG application,Stumbled upon RAP...</td>\n",
       "      <td>[RAG application, RAPTOR, segmenting content]</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.linkedin.com/posts/jobanpreet-sing...</td>\n",
       "      <td>üé§üîé Unveiling the Essentials of...</td>\n",
       "      <td>[Whisper, Wav2Vec2.0, Log-mel Spectrogram]</td>\n",
       "      <td>Whisper has been trained on 680,000 hours of a...</td>\n",
       "      <td>Whisper model trained on refined annotated dat...</td>\n",
       "      <td>[Whisper, Wav2Vec2.0, Log-mel Spectrogram]</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.linkedin.com/posts/jobanpreet-sing...</td>\n",
       "      <td>Successfully Finished My Proje...</td>\n",
       "      <td>[English to Punjabi, encoder-decoder framework...</td>\n",
       "      <td>Successfully Finished My Project on Converting...</td>\n",
       "      <td>Sophisticated encoder-decoder framework with d...</td>\n",
       "      <td>[English to Punjabi, encoder-decoder framework...</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.linkedin.com/posts/mohamedsalama1_...</td>\n",
       "      <td>Introducing ReFT: Revolutioniz...</td>\n",
       "      <td>[ReFT, LLM Fine-Tuning, Large Language Models]</td>\n",
       "      <td>ReFT enhances LLMs by applying a warm-up phase...</td>\n",
       "      <td>ReFT is a groundbreaking method to boost the l...</td>\n",
       "      <td>[ReFT, LLM Fine-Tuning, Large Language Models]</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.linkedin.com/posts/bavalpreet-sing...</td>\n",
       "      <td>#Google has recently unveiled #Gemma, a ground...</td>\n",
       "      <td>[Gemma, models, technology]</td>\n",
       "      <td>Gemma is available in two models: 2B and 7B,Ge...</td>\n",
       "      <td>Model Variants: Gemma is available in two mode...</td>\n",
       "      <td>[Gemma, features of Gemma, Gemma‚Äôs architecture ]</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.linkedin.com/posts/vasanthengineer...</td>\n",
       "      <td>Grok - 1, the newest MOE model boasting a 31B ...</td>\n",
       "      <td>[Grok - 1, MOE model, 31B parameter size]</td>\n",
       "      <td>A 314B parameter MOE model with 86B operationa...</td>\n",
       "      <td>A 314B parameter MOE model with 86B operationa...</td>\n",
       "      <td>[Grok - 1, MOE model, 31B parameter size]</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.linkedin.com/posts/vasanthengineer...</td>\n",
       "      <td>Curious about linking your Large Language Mode...</td>\n",
       "      <td>[Large Language Models, LLMs, CSV]</td>\n",
       "      <td>Convert a CSV file into an SQL schema,Present ...</td>\n",
       "      <td>Utilizing the Groq Inference Engine to operate...</td>\n",
       "      <td>[Large Language Models, LLMs, CSV]</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.linkedin.com/posts/vasanthengineer...</td>\n",
       "      <td>Following the unveiling of the groundbreaking ...</td>\n",
       "      <td>[Devin AI, Cognition, job threat]</td>\n",
       "      <td>Devin AI unveiling by Cognition,Understanding ...</td>\n",
       "      <td>Devin AI unveiling by Cognition,Comprehensive ...</td>\n",
       "      <td>[Devin AI, Cognition, job threat]</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average_jaccard_similarity</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         Link  \\\n",
       "0                           https://www.linkedin.com/posts/sahibpreetsingh...   \n",
       "1                           https://www.linkedin.com/posts/sahibpreetsingh...   \n",
       "2                           https://www.linkedin.com/posts/sahibpreetsingh...   \n",
       "3                           https://www.linkedin.com/posts/jobanpreet-sing...   \n",
       "4                           https://www.linkedin.com/posts/jobanpreet-sing...   \n",
       "5                           https://www.linkedin.com/posts/mohamedsalama1_...   \n",
       "6                           https://www.linkedin.com/posts/bavalpreet-sing...   \n",
       "7                           https://www.linkedin.com/posts/vasanthengineer...   \n",
       "8                           https://www.linkedin.com/posts/vasanthengineer...   \n",
       "9                           https://www.linkedin.com/posts/vasanthengineer...   \n",
       "Average_jaccard_similarity                                                NaN   \n",
       "\n",
       "                                                             Paraphrased Post  \\\n",
       "0                           Hello LLM Enthusiasts,\\nStarting a New Day wit...   \n",
       "1                           Hello, ùóüùóüùó† Enthusiasts, Get Ready for an Adven...   \n",
       "2                                           ùóüùóîùó†ùóî is mastering üêë beneath a ...   \n",
       "3                                           üé§üîé Unveiling the Essentials of...   \n",
       "4                                           Successfully Finished My Proje...   \n",
       "5                                           Introducing ReFT: Revolutioniz...   \n",
       "6                           #Google has recently unveiled #Gemma, a ground...   \n",
       "7                           Grok - 1, the newest MOE model boasting a 31B ...   \n",
       "8                           Curious about linking your Large Language Mode...   \n",
       "9                           Following the unveiling of the groundbreaking ...   \n",
       "Average_jaccard_similarity                                                NaN   \n",
       "\n",
       "                                                                     Keywords  \\\n",
       "0                                         [Semantic Chunking, Retrieval, RAG]   \n",
       "1                             [LLM Enthusiasts, Judge, LLM-generated answers]   \n",
       "2                               [RAG application, RAPTOR, segmenting content]   \n",
       "3                                  [Whisper, Wav2Vec2.0, Log-mel Spectrogram]   \n",
       "4                           [English to Punjabi, encoder-decoder framework...   \n",
       "5                              [ReFT, LLM Fine-Tuning, Large Language Models]   \n",
       "6                                                 [Gemma, models, technology]   \n",
       "7                                   [Grok - 1, MOE model, 31B parameter size]   \n",
       "8                                          [Large Language Models, LLMs, CSV]   \n",
       "9                                           [Devin AI, Cognition, job threat]   \n",
       "Average_jaccard_similarity                                                NaN   \n",
       "\n",
       "                                                                   Take Aways  \\\n",
       "0                           Chunking involves dividing a document into sma...   \n",
       "1                           LLMs serving as judges,Utilizing N answers fro...   \n",
       "2                           Consider experimenting with algorithms like TS...   \n",
       "3                           Whisper has been trained on 680,000 hours of a...   \n",
       "4                           Successfully Finished My Project on Converting...   \n",
       "5                           ReFT enhances LLMs by applying a warm-up phase...   \n",
       "6                           Gemma is available in two models: 2B and 7B,Ge...   \n",
       "7                           A 314B parameter MOE model with 86B operationa...   \n",
       "8                           Convert a CSV file into an SQL schema,Present ...   \n",
       "9                           Devin AI unveiling by Cognition,Understanding ...   \n",
       "Average_jaccard_similarity                                                NaN   \n",
       "\n",
       "                                                                   Highlights  \\\n",
       "0                           How can we break down intricate user inquiries...   \n",
       "1                           LLMs ascending to the role of a Judge,Challeng...   \n",
       "2                           Developing a RAG application,Stumbled upon RAP...   \n",
       "3                           Whisper model trained on refined annotated dat...   \n",
       "4                           Sophisticated encoder-decoder framework with d...   \n",
       "5                           ReFT is a groundbreaking method to boost the l...   \n",
       "6                           Model Variants: Gemma is available in two mode...   \n",
       "7                           A 314B parameter MOE model with 86B operationa...   \n",
       "8                           Utilizing the Groq Inference Engine to operate...   \n",
       "9                           Devin AI unveiling by Cognition,Comprehensive ...   \n",
       "Average_jaccard_similarity                                                NaN   \n",
       "\n",
       "                                                         GroundTruth Keywords  \\\n",
       "0                                         [Semantic Chunking, Retrieval, RAG]   \n",
       "1                                        [LLMs, Judge, Evaluation challenges]   \n",
       "2                               [RAG application, RAPTOR, segmenting content]   \n",
       "3                                  [Whisper, Wav2Vec2.0, Log-mel Spectrogram]   \n",
       "4                           [English to Punjabi, encoder-decoder framework...   \n",
       "5                              [ReFT, LLM Fine-Tuning, Large Language Models]   \n",
       "6                           [Gemma, features of Gemma, Gemma‚Äôs architecture ]   \n",
       "7                                   [Grok - 1, MOE model, 31B parameter size]   \n",
       "8                                          [Large Language Models, LLMs, CSV]   \n",
       "9                                           [Devin AI, Cognition, job threat]   \n",
       "Average_jaccard_similarity                                                NaN   \n",
       "\n",
       "                            jaccard_similarity  \n",
       "0                                         1.00  \n",
       "1                                         0.20  \n",
       "2                                         1.00  \n",
       "3                                         1.00  \n",
       "4                                         0.50  \n",
       "5                                         1.00  \n",
       "6                                         0.20  \n",
       "7                                         1.00  \n",
       "8                                         1.00  \n",
       "9                                         1.00  \n",
       "Average_jaccard_similarity                0.79  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de872654-c36c-4d15-852b-c68a871be1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65733d14-aff1-431e-9792-26cc0e35eb28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7436cd-a0c4-4ae8-ac3a-0d966530aff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6947985-a7a0-4aa7-81d1-645a6446346f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa70adb5-348f-455d-96be-7b4ceb21f9bb",
   "metadata": {},
   "source": [
    "# Calculating Embedding Distance (Using HuggingFace Embeddings)\n",
    "\n",
    "#### Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "02ec6d57-4154-484f-bd65-69e87ea99650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"results.csv\")\n",
    "\n",
    "df=df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a36fa9c-a7e8-4169-aeb4-5a346f4ef427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d777aae1-aa66-48f6-bd17-7b41680882fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.evaluation import EmbeddingDistance\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "hf_evaluator = load_evaluator(\"embedding_distance\", embeddings=embedding_model,distance_metric=EmbeddingDistance.EUCLIDEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4fc9cf8f-5160-4159-acdf-ce626cf48fbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingDistanceEvalChain(embeddings=HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='thenlper/gte-small', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False), distance_metric=<EmbeddingDistance.EUCLIDEAN: 'euclidean'>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615fb38d-24a9-4ad1-8127-74983930f3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0ccb9f6a-c04e-4779-9229-7c878e4e3fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Semantic Chunking,Retrieval,RAG]\n",
      "[Semantic Chunking,Retrieval,RAG]\n",
      "0.0\n",
      "[LLM Enthusiasts,Judge,LLM-generated answers]\n",
      "[LLMs,Judge,Evaluation challenges]\n",
      "0.4578206594545731\n",
      "[RAG application,RAPTOR,segmenting content]\n",
      "[RAG application,RAPTOR,segmenting content]\n",
      "0.0\n",
      "[Whisper,Wav2Vec2.0,Log-mel Spectrogram]\n",
      "[Whisper,Wav2Vec2.0,Log-mel Spectrogram]\n",
      "0.0\n",
      "[English to Punjabi,encoder-decoder framework, LSTM units]\n",
      "[English to Punjabi,encoder-decoder framework,Word Accuracy]\n",
      "0.1120324269360636\n",
      "[ReFT,LLM Fine-Tuning,Large Language Models]\n",
      "[ReFT,LLM Fine-Tuning,Large Language Models]\n",
      "0.0\n",
      "[Gemma,models,technology]\n",
      "[Gemma,features of Gemma,Gemma‚Äôs architecture]\n",
      "0.3981857970858412\n",
      "[Grok - 1,MOE model,31B parameter size]\n",
      "[Grok - 1,MOE model,31B parameter size]\n",
      "0.0\n",
      "[Large Language Models,LLMs,CSV]\n",
      "[Large Language Models,LLMs,CSV]\n",
      "0.0\n",
      "[Devin AI,Cognition,job threat]\n",
      "[Devin AI,Cognition,job threat]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Empty list to store similarity scores\n",
    "similarity_scores = []\n",
    "\n",
    "# Iterate over each row of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get keywords and ground truth keywords from the current row\n",
    "    keywords = row['Keywords']\n",
    "    print(keywords)\n",
    "    ground_truth_keywords = row['GroundTruth Keywords']\n",
    "    print(ground_truth_keywords)\n",
    "    \n",
    "    # Calculate similarity score for each keyword pair\n",
    "    scores = []\n",
    "    for keyword, gt_keyword in zip(keywords, ground_truth_keywords):\n",
    "        score = hf_evaluator.evaluate_strings(prediction=keyword, reference=gt_keyword)['score']\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Take the average of similarity scores for all keyword pairs\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(average_score)\n",
    "    \n",
    "    # Append the average similarity score to the list\n",
    "    similarity_scores.append(average_score)\n",
    "\n",
    "# Add the similarity scores to the DataFrame as a new column\n",
    "df['Euclidean_Distance'] = similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d5d263de-fd11-4d1d-a73c-fc13f0665301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Link</th>\n",
       "      <th>Paraphrased Post</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Take Aways</th>\n",
       "      <th>Highlights</th>\n",
       "      <th>GroundTruth Keywords</th>\n",
       "      <th>jaccard_similarity</th>\n",
       "      <th>Euclidean_Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.linkedin.com/posts/sahibpreetsingh...</td>\n",
       "      <td>Hello LLM Enthusiasts,\\nStarting a New Day wit...</td>\n",
       "      <td>[Semantic Chunking,Retrieval,RAG]</td>\n",
       "      <td>Chunking involves dividing a document into sma...</td>\n",
       "      <td>How can we break down intricate user inquiries...</td>\n",
       "      <td>[Semantic Chunking,Retrieval,RAG]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.linkedin.com/posts/sahibpreetsingh...</td>\n",
       "      <td>Hello, ùóüùóüùó† Enthusiasts, Get Ready for an Adven...</td>\n",
       "      <td>[LLM Enthusiasts,Judge,LLM-generated answers]</td>\n",
       "      <td>LLMs serving as judges,Utilizing N answers fro...</td>\n",
       "      <td>LLMs ascending to the role of a Judge,Challeng...</td>\n",
       "      <td>[LLMs,Judge,Evaluation challenges]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.457821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.linkedin.com/posts/sahibpreetsingh...</td>\n",
       "      <td>ùóüùóîùó†ùóî is mastering üêë beneath a ...</td>\n",
       "      <td>[RAG application,RAPTOR,segmenting content]</td>\n",
       "      <td>Consider experimenting with algorithms like TS...</td>\n",
       "      <td>Developing a RAG application,Stumbled upon RAP...</td>\n",
       "      <td>[RAG application,RAPTOR,segmenting content]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.linkedin.com/posts/jobanpreet-sing...</td>\n",
       "      <td>üé§üîé Unveiling the Essentials of...</td>\n",
       "      <td>[Whisper,Wav2Vec2.0,Log-mel Spectrogram]</td>\n",
       "      <td>Whisper has been trained on 680,000 hours of a...</td>\n",
       "      <td>Whisper model trained on refined annotated dat...</td>\n",
       "      <td>[Whisper,Wav2Vec2.0,Log-mel Spectrogram]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.linkedin.com/posts/jobanpreet-sing...</td>\n",
       "      <td>Successfully Finished My Proje...</td>\n",
       "      <td>[English to Punjabi,encoder-decoder framework,...</td>\n",
       "      <td>Successfully Finished My Project on Converting...</td>\n",
       "      <td>Sophisticated encoder-decoder framework with d...</td>\n",
       "      <td>[English to Punjabi,encoder-decoder framework,...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.112032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>https://www.linkedin.com/posts/mohamedsalama1_...</td>\n",
       "      <td>Introducing ReFT: Revolutioniz...</td>\n",
       "      <td>[ReFT,LLM Fine-Tuning,Large Language Models]</td>\n",
       "      <td>ReFT enhances LLMs by applying a warm-up phase...</td>\n",
       "      <td>ReFT is a groundbreaking method to boost the l...</td>\n",
       "      <td>[ReFT,LLM Fine-Tuning,Large Language Models]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>https://www.linkedin.com/posts/bavalpreet-sing...</td>\n",
       "      <td>#Google has recently unveiled #Gemma, a ground...</td>\n",
       "      <td>[Gemma,models,technology]</td>\n",
       "      <td>Gemma is available in two models: 2B and 7B,Ge...</td>\n",
       "      <td>Model Variants: Gemma is available in two mode...</td>\n",
       "      <td>[Gemma,features of Gemma,Gemma‚Äôs architecture]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.398186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>https://www.linkedin.com/posts/vasanthengineer...</td>\n",
       "      <td>Grok - 1, the newest MOE model boasting a 31B ...</td>\n",
       "      <td>[Grok - 1,MOE model,31B parameter size]</td>\n",
       "      <td>A 314B parameter MOE model with 86B operationa...</td>\n",
       "      <td>A 314B parameter MOE model with 86B operationa...</td>\n",
       "      <td>[Grok - 1,MOE model,31B parameter size]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>https://www.linkedin.com/posts/vasanthengineer...</td>\n",
       "      <td>Curious about linking your Large Language Mode...</td>\n",
       "      <td>[Large Language Models,LLMs,CSV]</td>\n",
       "      <td>Convert a CSV file into an SQL schema,Present ...</td>\n",
       "      <td>Utilizing the Groq Inference Engine to operate...</td>\n",
       "      <td>[Large Language Models,LLMs,CSV]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>https://www.linkedin.com/posts/vasanthengineer...</td>\n",
       "      <td>Following the unveiling of the groundbreaking ...</td>\n",
       "      <td>[Devin AI,Cognition,job threat]</td>\n",
       "      <td>Devin AI unveiling by Cognition,Understanding ...</td>\n",
       "      <td>Devin AI unveiling by Cognition,Comprehensive ...</td>\n",
       "      <td>[Devin AI,Cognition,job threat]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                                               Link  \\\n",
       "0          0  https://www.linkedin.com/posts/sahibpreetsingh...   \n",
       "1          1  https://www.linkedin.com/posts/sahibpreetsingh...   \n",
       "2          2  https://www.linkedin.com/posts/sahibpreetsingh...   \n",
       "3          3  https://www.linkedin.com/posts/jobanpreet-sing...   \n",
       "4          4  https://www.linkedin.com/posts/jobanpreet-sing...   \n",
       "5          5  https://www.linkedin.com/posts/mohamedsalama1_...   \n",
       "6          6  https://www.linkedin.com/posts/bavalpreet-sing...   \n",
       "7          7  https://www.linkedin.com/posts/vasanthengineer...   \n",
       "8          8  https://www.linkedin.com/posts/vasanthengineer...   \n",
       "9          9  https://www.linkedin.com/posts/vasanthengineer...   \n",
       "\n",
       "                                    Paraphrased Post  \\\n",
       "0  Hello LLM Enthusiasts,\\nStarting a New Day wit...   \n",
       "1  Hello, ùóüùóüùó† Enthusiasts, Get Ready for an Adven...   \n",
       "2                  ùóüùóîùó†ùóî is mastering üêë beneath a ...   \n",
       "3                  üé§üîé Unveiling the Essentials of...   \n",
       "4                  Successfully Finished My Proje...   \n",
       "5                  Introducing ReFT: Revolutioniz...   \n",
       "6  #Google has recently unveiled #Gemma, a ground...   \n",
       "7  Grok - 1, the newest MOE model boasting a 31B ...   \n",
       "8  Curious about linking your Large Language Mode...   \n",
       "9  Following the unveiling of the groundbreaking ...   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0                  [Semantic Chunking,Retrieval,RAG]   \n",
       "1      [LLM Enthusiasts,Judge,LLM-generated answers]   \n",
       "2        [RAG application,RAPTOR,segmenting content]   \n",
       "3           [Whisper,Wav2Vec2.0,Log-mel Spectrogram]   \n",
       "4  [English to Punjabi,encoder-decoder framework,...   \n",
       "5       [ReFT,LLM Fine-Tuning,Large Language Models]   \n",
       "6                          [Gemma,models,technology]   \n",
       "7            [Grok - 1,MOE model,31B parameter size]   \n",
       "8                   [Large Language Models,LLMs,CSV]   \n",
       "9                    [Devin AI,Cognition,job threat]   \n",
       "\n",
       "                                          Take Aways  \\\n",
       "0  Chunking involves dividing a document into sma...   \n",
       "1  LLMs serving as judges,Utilizing N answers fro...   \n",
       "2  Consider experimenting with algorithms like TS...   \n",
       "3  Whisper has been trained on 680,000 hours of a...   \n",
       "4  Successfully Finished My Project on Converting...   \n",
       "5  ReFT enhances LLMs by applying a warm-up phase...   \n",
       "6  Gemma is available in two models: 2B and 7B,Ge...   \n",
       "7  A 314B parameter MOE model with 86B operationa...   \n",
       "8  Convert a CSV file into an SQL schema,Present ...   \n",
       "9  Devin AI unveiling by Cognition,Understanding ...   \n",
       "\n",
       "                                          Highlights  \\\n",
       "0  How can we break down intricate user inquiries...   \n",
       "1  LLMs ascending to the role of a Judge,Challeng...   \n",
       "2  Developing a RAG application,Stumbled upon RAP...   \n",
       "3  Whisper model trained on refined annotated dat...   \n",
       "4  Sophisticated encoder-decoder framework with d...   \n",
       "5  ReFT is a groundbreaking method to boost the l...   \n",
       "6  Model Variants: Gemma is available in two mode...   \n",
       "7  A 314B parameter MOE model with 86B operationa...   \n",
       "8  Utilizing the Groq Inference Engine to operate...   \n",
       "9  Devin AI unveiling by Cognition,Comprehensive ...   \n",
       "\n",
       "                                GroundTruth Keywords  jaccard_similarity  \\\n",
       "0                  [Semantic Chunking,Retrieval,RAG]                 1.0   \n",
       "1                 [LLMs,Judge,Evaluation challenges]                 0.2   \n",
       "2        [RAG application,RAPTOR,segmenting content]                 1.0   \n",
       "3           [Whisper,Wav2Vec2.0,Log-mel Spectrogram]                 1.0   \n",
       "4  [English to Punjabi,encoder-decoder framework,...                 0.5   \n",
       "5       [ReFT,LLM Fine-Tuning,Large Language Models]                 1.0   \n",
       "6     [Gemma,features of Gemma,Gemma‚Äôs architecture]                 0.2   \n",
       "7            [Grok - 1,MOE model,31B parameter size]                 1.0   \n",
       "8                   [Large Language Models,LLMs,CSV]                 1.0   \n",
       "9                    [Devin AI,Cognition,job threat]                 1.0   \n",
       "\n",
       "   Euclidean_Distance  \n",
       "0            0.000000  \n",
       "1            0.457821  \n",
       "2            0.000000  \n",
       "3            0.000000  \n",
       "4            0.112032  \n",
       "5            0.000000  \n",
       "6            0.398186  \n",
       "7            0.000000  \n",
       "8            0.000000  \n",
       "9            0.000000  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7487b12-4abc-4a56-974d-5ee25a02f240",
   "metadata": {},
   "source": [
    "## Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "421f2b55-9f71-4905-9bb7-c63a174a0e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Semantic Chunking,Retrieval,RAG]\n",
      "[Semantic Chunking,Retrieval,RAG]\n",
      "-0.0\n",
      "[LLM Enthusiasts,Judge,LLM-generated answers]\n",
      "[LLMs,Judge,Evaluation challenges]\n",
      "0.127552\n",
      "[RAG application,RAPTOR,segmenting content]\n",
      "[RAG application,RAPTOR,segmenting content]\n",
      "0.0\n",
      "[Whisper,Wav2Vec2.0,Log-mel Spectrogram]\n",
      "[Whisper,Wav2Vec2.0,Log-mel Spectrogram]\n",
      "-0.0\n",
      "[English to Punjabi,encoder-decoder framework, LSTM units]\n",
      "[English to Punjabi,encoder-decoder framework,Word Accuracy]\n",
      "0.030446\n",
      "[ReFT,LLM Fine-Tuning,Large Language Models]\n",
      "[ReFT,LLM Fine-Tuning,Large Language Models]\n",
      "0.0\n",
      "[Gemma,models,technology]\n",
      "[Gemma,features of Gemma,Gemma‚Äôs architecture]\n",
      "0.110311\n",
      "[Grok - 1,MOE model,31B parameter size]\n",
      "[Grok - 1,MOE model,31B parameter size]\n",
      "0.0\n",
      "[Large Language Models,LLMs,CSV]\n",
      "[Large Language Models,LLMs,CSV]\n",
      "-0.0\n",
      "[Devin AI,Cognition,job threat]\n",
      "[Devin AI,Cognition,job threat]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "hf_evaluator = load_evaluator(\"embedding_distance\", embeddings=embedding_model) # For cosine similarity \n",
    "rounded_scores = []\n",
    "\n",
    "# Iterate over each row of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get keywords and ground truth keywords from the current row\n",
    "    keywords = row['Keywords']\n",
    "    print(keywords)\n",
    "    ground_truth_keywords = row['GroundTruth Keywords']\n",
    "    print(ground_truth_keywords)\n",
    "    \n",
    "    # Calculate similarity score for each keyword pair\n",
    "    scores = []\n",
    "    for keyword, gt_keyword in zip(keywords, ground_truth_keywords):\n",
    "        score = hf_evaluator.evaluate_strings(prediction=keyword, reference=gt_keyword)['score']\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Take the average of similarity scores for all keyword pairs\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    \n",
    "    \n",
    "    # Round the average score to a certain number of decimal places\n",
    "    rounded_score = round(average_score, 6)  # You can adjust the number of decimal places as needed\n",
    "    print(rounded_score)\n",
    "    # Append the rounded similarity score to the list\n",
    "    rounded_scores.append(rounded_score)\n",
    "\n",
    "# Add the rounded similarity scores to the DataFrame as a new column\n",
    "df['Cosine Similarity'] = rounded_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b5e99102-5d2a-41b1-b093-b5ea3f7988b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Link</th>\n",
       "      <th>Paraphrased Post</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Take Aways</th>\n",
       "      <th>Highlights</th>\n",
       "      <th>GroundTruth Keywords</th>\n",
       "      <th>jaccard_similarity</th>\n",
       "      <th>Euclidean_Distance</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.linkedin.com/posts/sahibpreetsingh...</td>\n",
       "      <td>Hello LLM Enthusiasts,\\nStarting a New Day wit...</td>\n",
       "      <td>[Semantic Chunking,Retrieval,RAG]</td>\n",
       "      <td>Chunking involves dividing a document into sma...</td>\n",
       "      <td>How can we break down intricate user inquiries...</td>\n",
       "      <td>[Semantic Chunking,Retrieval,RAG]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.linkedin.com/posts/sahibpreetsingh...</td>\n",
       "      <td>Hello, ùóüùóüùó† Enthusiasts, Get Ready for an Adven...</td>\n",
       "      <td>[LLM Enthusiasts,Judge,LLM-generated answers]</td>\n",
       "      <td>LLMs serving as judges,Utilizing N answers fro...</td>\n",
       "      <td>LLMs ascending to the role of a Judge,Challeng...</td>\n",
       "      <td>[LLMs,Judge,Evaluation challenges]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.457821</td>\n",
       "      <td>0.127552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.linkedin.com/posts/sahibpreetsingh...</td>\n",
       "      <td>ùóüùóîùó†ùóî is mastering üêë beneath a ...</td>\n",
       "      <td>[RAG application,RAPTOR,segmenting content]</td>\n",
       "      <td>Consider experimenting with algorithms like TS...</td>\n",
       "      <td>Developing a RAG application,Stumbled upon RAP...</td>\n",
       "      <td>[RAG application,RAPTOR,segmenting content]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.linkedin.com/posts/jobanpreet-sing...</td>\n",
       "      <td>üé§üîé Unveiling the Essentials of...</td>\n",
       "      <td>[Whisper,Wav2Vec2.0,Log-mel Spectrogram]</td>\n",
       "      <td>Whisper has been trained on 680,000 hours of a...</td>\n",
       "      <td>Whisper model trained on refined annotated dat...</td>\n",
       "      <td>[Whisper,Wav2Vec2.0,Log-mel Spectrogram]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.linkedin.com/posts/jobanpreet-sing...</td>\n",
       "      <td>Successfully Finished My Proje...</td>\n",
       "      <td>[English to Punjabi,encoder-decoder framework,...</td>\n",
       "      <td>Successfully Finished My Project on Converting...</td>\n",
       "      <td>Sophisticated encoder-decoder framework with d...</td>\n",
       "      <td>[English to Punjabi,encoder-decoder framework,...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.112032</td>\n",
       "      <td>0.030446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>https://www.linkedin.com/posts/mohamedsalama1_...</td>\n",
       "      <td>Introducing ReFT: Revolutioniz...</td>\n",
       "      <td>[ReFT,LLM Fine-Tuning,Large Language Models]</td>\n",
       "      <td>ReFT enhances LLMs by applying a warm-up phase...</td>\n",
       "      <td>ReFT is a groundbreaking method to boost the l...</td>\n",
       "      <td>[ReFT,LLM Fine-Tuning,Large Language Models]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>https://www.linkedin.com/posts/bavalpreet-sing...</td>\n",
       "      <td>#Google has recently unveiled #Gemma, a ground...</td>\n",
       "      <td>[Gemma,models,technology]</td>\n",
       "      <td>Gemma is available in two models: 2B and 7B,Ge...</td>\n",
       "      <td>Model Variants: Gemma is available in two mode...</td>\n",
       "      <td>[Gemma,features of Gemma,Gemma‚Äôs architecture]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.398186</td>\n",
       "      <td>0.110311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>https://www.linkedin.com/posts/vasanthengineer...</td>\n",
       "      <td>Grok - 1, the newest MOE model boasting a 31B ...</td>\n",
       "      <td>[Grok - 1,MOE model,31B parameter size]</td>\n",
       "      <td>A 314B parameter MOE model with 86B operationa...</td>\n",
       "      <td>A 314B parameter MOE model with 86B operationa...</td>\n",
       "      <td>[Grok - 1,MOE model,31B parameter size]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>https://www.linkedin.com/posts/vasanthengineer...</td>\n",
       "      <td>Curious about linking your Large Language Mode...</td>\n",
       "      <td>[Large Language Models,LLMs,CSV]</td>\n",
       "      <td>Convert a CSV file into an SQL schema,Present ...</td>\n",
       "      <td>Utilizing the Groq Inference Engine to operate...</td>\n",
       "      <td>[Large Language Models,LLMs,CSV]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>https://www.linkedin.com/posts/vasanthengineer...</td>\n",
       "      <td>Following the unveiling of the groundbreaking ...</td>\n",
       "      <td>[Devin AI,Cognition,job threat]</td>\n",
       "      <td>Devin AI unveiling by Cognition,Understanding ...</td>\n",
       "      <td>Devin AI unveiling by Cognition,Comprehensive ...</td>\n",
       "      <td>[Devin AI,Cognition,job threat]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                                               Link  \\\n",
       "0          0  https://www.linkedin.com/posts/sahibpreetsingh...   \n",
       "1          1  https://www.linkedin.com/posts/sahibpreetsingh...   \n",
       "2          2  https://www.linkedin.com/posts/sahibpreetsingh...   \n",
       "3          3  https://www.linkedin.com/posts/jobanpreet-sing...   \n",
       "4          4  https://www.linkedin.com/posts/jobanpreet-sing...   \n",
       "5          5  https://www.linkedin.com/posts/mohamedsalama1_...   \n",
       "6          6  https://www.linkedin.com/posts/bavalpreet-sing...   \n",
       "7          7  https://www.linkedin.com/posts/vasanthengineer...   \n",
       "8          8  https://www.linkedin.com/posts/vasanthengineer...   \n",
       "9          9  https://www.linkedin.com/posts/vasanthengineer...   \n",
       "\n",
       "                                    Paraphrased Post  \\\n",
       "0  Hello LLM Enthusiasts,\\nStarting a New Day wit...   \n",
       "1  Hello, ùóüùóüùó† Enthusiasts, Get Ready for an Adven...   \n",
       "2                  ùóüùóîùó†ùóî is mastering üêë beneath a ...   \n",
       "3                  üé§üîé Unveiling the Essentials of...   \n",
       "4                  Successfully Finished My Proje...   \n",
       "5                  Introducing ReFT: Revolutioniz...   \n",
       "6  #Google has recently unveiled #Gemma, a ground...   \n",
       "7  Grok - 1, the newest MOE model boasting a 31B ...   \n",
       "8  Curious about linking your Large Language Mode...   \n",
       "9  Following the unveiling of the groundbreaking ...   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0                  [Semantic Chunking,Retrieval,RAG]   \n",
       "1      [LLM Enthusiasts,Judge,LLM-generated answers]   \n",
       "2        [RAG application,RAPTOR,segmenting content]   \n",
       "3           [Whisper,Wav2Vec2.0,Log-mel Spectrogram]   \n",
       "4  [English to Punjabi,encoder-decoder framework,...   \n",
       "5       [ReFT,LLM Fine-Tuning,Large Language Models]   \n",
       "6                          [Gemma,models,technology]   \n",
       "7            [Grok - 1,MOE model,31B parameter size]   \n",
       "8                   [Large Language Models,LLMs,CSV]   \n",
       "9                    [Devin AI,Cognition,job threat]   \n",
       "\n",
       "                                          Take Aways  \\\n",
       "0  Chunking involves dividing a document into sma...   \n",
       "1  LLMs serving as judges,Utilizing N answers fro...   \n",
       "2  Consider experimenting with algorithms like TS...   \n",
       "3  Whisper has been trained on 680,000 hours of a...   \n",
       "4  Successfully Finished My Project on Converting...   \n",
       "5  ReFT enhances LLMs by applying a warm-up phase...   \n",
       "6  Gemma is available in two models: 2B and 7B,Ge...   \n",
       "7  A 314B parameter MOE model with 86B operationa...   \n",
       "8  Convert a CSV file into an SQL schema,Present ...   \n",
       "9  Devin AI unveiling by Cognition,Understanding ...   \n",
       "\n",
       "                                          Highlights  \\\n",
       "0  How can we break down intricate user inquiries...   \n",
       "1  LLMs ascending to the role of a Judge,Challeng...   \n",
       "2  Developing a RAG application,Stumbled upon RAP...   \n",
       "3  Whisper model trained on refined annotated dat...   \n",
       "4  Sophisticated encoder-decoder framework with d...   \n",
       "5  ReFT is a groundbreaking method to boost the l...   \n",
       "6  Model Variants: Gemma is available in two mode...   \n",
       "7  A 314B parameter MOE model with 86B operationa...   \n",
       "8  Utilizing the Groq Inference Engine to operate...   \n",
       "9  Devin AI unveiling by Cognition,Comprehensive ...   \n",
       "\n",
       "                                GroundTruth Keywords  jaccard_similarity  \\\n",
       "0                  [Semantic Chunking,Retrieval,RAG]                 1.0   \n",
       "1                 [LLMs,Judge,Evaluation challenges]                 0.2   \n",
       "2        [RAG application,RAPTOR,segmenting content]                 1.0   \n",
       "3           [Whisper,Wav2Vec2.0,Log-mel Spectrogram]                 1.0   \n",
       "4  [English to Punjabi,encoder-decoder framework,...                 0.5   \n",
       "5       [ReFT,LLM Fine-Tuning,Large Language Models]                 1.0   \n",
       "6     [Gemma,features of Gemma,Gemma‚Äôs architecture]                 0.2   \n",
       "7            [Grok - 1,MOE model,31B parameter size]                 1.0   \n",
       "8                   [Large Language Models,LLMs,CSV]                 1.0   \n",
       "9                    [Devin AI,Cognition,job threat]                 1.0   \n",
       "\n",
       "   Euclidean_Distance  Cosine Similarity  \n",
       "0            0.000000          -0.000000  \n",
       "1            0.457821           0.127552  \n",
       "2            0.000000           0.000000  \n",
       "3            0.000000          -0.000000  \n",
       "4            0.112032           0.030446  \n",
       "5            0.000000           0.000000  \n",
       "6            0.398186           0.110311  \n",
       "7            0.000000           0.000000  \n",
       "8            0.000000          -0.000000  \n",
       "9            0.000000           0.000000  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a98b6314-3618-4f3a-a77f-19f29437ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f5f248eb-ff71-44ed-a34c-4baf4338063c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09680388834764779"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Euclidean_Distance\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cc60ff56-ca38-45b1-93d4-a6040e5aad54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0268309"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Cosine Similarity\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bf260838-bca3-4fc8-833a-cce0faffb114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"jaccard_similarity\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179a8bda-7c12-4bf5-9d0c-72f23717e76c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
