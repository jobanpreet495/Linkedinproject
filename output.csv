Link,Paraphrased Post,Keywords,Take Aways,Highlights,GroundTruth Keywords,jaccard_similarity,Euclidean_Distance,Cosine Similarity
https://www.linkedin.com/posts/sahibpreetsinghh_hey-llm-peeps-new-day-sharing-new-learning-activity-7177861772039651328-BMBV?utm_source=share&utm_medium=member_desktop,"Hello LLM Enthusiasts,
Starting a New Day with Fresh Insights.
When constructing RAG, we encounter two primary challenges:
1. How can we break down intricate user inquiries into manageable segments and then address each sub-question using RAG?
2. Which retrieval method proves to be the most effective?
Let's delve deeper.

Q- What does Chunking entail?

It involves dividing a document into smaller sections and then generating embeddings for these sections, enabling the language model to provide the most suitable response from your extensive document.

🧩 Semantic Chunking
•🔍 Unraveling Semantic Chunking for Enhanced Text Analysis 🧩

🔹 Setting the Window Size: Opt for a window size (e.g., W = 3) to divide the document into smaller sentence groups, creating overlapping sections.
🔹 Generating Group Embeddings: Produce embeddings for each section to encapsulate its semantic core.
🔹 Assessing Semantic Similarity: Use semantic similarity scores to gauge the resemblance between consecutive sections.
🔹 Identifying Split Points: Employ methods like percentile or standard deviation-based criteria to pinpoint ideal division points within the document.
🔹 Implementing Document Division: Segment the document into meaningful sections based on the established criteria.

Now, we have a clear understanding of Semantic Chunking.

What entails Retrieval and which strategy should we opt for?

Retrieval involves sourcing text segments that serve as the most relevant answers to a user query, often from a Vector Database but sometimes from an Index on Local Storage. For this piece, I'm utilizing Raptor as a Retrieval Strategy (Due to LinkedIn's word limit, I can't delve into this here :) )

For more details, check out my previous post - Visit my profile.

In this brief guide, I've merged Semantic Chunking with Raptor for Retrieval, and the outcomes are promising (Having tested this blend on my own project, I'm sharing a glimpse here).

Before signing off, I have a couple of inquiries:

1. Employing Raptor Retriever without a Vector Store, using only an Index and specifically VectorStoreIndex from LlamaIndex in ""tree_traversal"" mode, results in significant delays and no responses. If Ravi Theja Desetty, Logan Markewich, or anyone else could offer assistance, it would be greatly appreciated.

2. I'm also puzzled about the purpose of SimpleVectorStore if it lacks typical vector store features, as most fundamental functionalities found in Chroma, Weaviate, Pinecone, or LanceDB are absent in this Class.

I'd appreciate some clarification. Perhaps I'm looking at it from a different perspective.
Shoutout to friends and mentors - Vetrivel PS, Philip Vollet, VASANTH P, Harpreet Sahota 🥑, Mahesh Deshwal, Hetarth Chopra, Harbhajan Singh, Ravi Tanwar, Pramodith B., and Navdeep Singh.

For the code, click here - [Link]","[Semantic Chunking,Retrieval,RAG]","Chunking involves dividing a document into smaller sections and generating embeddings for them,Semantic Chunking involves setting window size, generating group embeddings, assessing semantic similarity, identifying split points, and implementing document division,Retrieval involves sourcing text segments from a Vector Database or Index on Local Storage, Raptor is a Retrieval Strategy","How can we break down intricate user inquiries into manageable segments and address each sub-question using RAG?,Chunking entails dividing a document into smaller sections and generating embeddings for them,Semantic Chunking involves setting window size, generating group embeddings, assessing semantic similarity, identifying split points, and implementing document division","[Semantic Chunking,Retrieval,RAG]",1,0,0
https://www.linkedin.com/posts/sahibpreetsinghh_hey-%3F%3F%3F-%3F%3F%3F%3F-exciting-times-ahead-activity-7176023810989568000-BNXv?utm_source=share&utm_medium=member_desktop,"Hello, 𝗟𝗟𝗠 Enthusiasts, Get Ready for an Adventure! 🚀 Dive into a world where LLMs ascend to the role of a Judge, tasked with evaluating the creations of their peers. 💼

Note - This innovative concept was brought to life by myself alongside my team members Jayita Bhattacharyya and Ameena Ansari for a competition hosted by traversaal.ai and Qdrant.

In this scenario, employing a 𝗛𝘂𝗺𝗮𝗻 𝗔𝗻𝗻𝗼𝘁𝗮𝘁𝗼𝗿 is an expensive endeavor. Hence, the idea of allowing LLMs to serve as judges emerges.

However, several challenges arise in evaluating LLM-generated answers, including:

1. Answers that are imaginative and factually incorrect.

2. Repetition of the same facts.

3. Lack of coherence.

Strategies to Overcome These Challenges: 🛠️

1. Utilize 𝗡 𝗮𝗻𝘀𝘄𝗲𝗿𝘀 from 𝗠 𝗵𝘂𝗺𝗮𝗻 𝗮𝗻𝗻𝗼𝘁𝗮𝘁𝗼𝗿𝘀 (with N being a reasonable figure like 30-50 and M being around 2-3).

2. Develop a clear prompt that outlines the criteria for judgment:

    2.1 Highlight the main points for evaluation.

    2.2 Introduce a simple rating scale (ideally 1-5) to streamline the LLM's review process. This scale can be expanded from 1-100 later, but starting simple is crucial for the LLM's tasks.

    2.3 Explain the importance of the scale (i.e., the difference between a 1 and a 5).

Important Points to Remember: 📌

1. 🎯 Aim for continuous improvement, acknowledging that perfection is unachievable.

2. 🧭 Use benchmarks for guidance.

3. 📚 Employ a few-shot examples to enhance understanding. 😊

Special thanks to Aymeric Roucher for highlighting this topic in a recent post - https://lnkd.in/g5RfJanY

Shoutout to friends and mentors - Vetrivel PS, VASANTH P, Mahesh Deshwal, Ravi Tanwar, Abhishek Das, Harbhajan Singh, Basant Singh, Abhishek Thakur, Philipp Schmid, and Sanyam Bhutani for your constant inspiration with your AI insights.

Link to our project's repository - https://lnkd.in/ezQcmgda","[LLM Enthusiasts,Judge,LLM-generated answers]","LLMs serving as judges,Utilizing N answers from M human annotators,Developing a clear prompt for judgment criteria","LLMs ascending to the role of a Judge,Challenges in evaluating LLM-generated answers,Strategies to overcome challenges","[LLMs,Judge,Evaluation challenges]",0.2,0.457820659454573,0.127552
https://www.linkedin.com/posts/sahibpreetsinghh_%3F%3F%3F%3F-is-learning-%3F%3F-%3F%3F%3F%3F%3F-%3F-activity-7173898136111841282-F8sV?utm_source=share&utm_medium=member_desktop,"                𝗟𝗔𝗠𝗔 is mastering 🐑 beneath a Shade 🌳
Lately, I've been engrossed in developing a RAG application, during which I stumbled upon RAPTOR, 🦖 an innovative technique for segmenting content, through my LinkedIn feed.

Here's a simplified explanation:

1. Begin by dividing a large text into, for instance, 50 segments.

2. Next, apply 𝗨𝗠𝗔𝗣, 🗺️ to reduce the dimensions of these segments. Following dimension reduction, a Gaussian Mixture model clusters the segments based on contextual similarities.

3. Each cluster's segments are then condensed into a new segment by an LLM.

4. Repeat steps 1-3 until no further clusters emerge.

However, the end result might not be exactly what we're aiming for. Throughout this process, segments are merged at various levels of detail, creating unique combinations that can be tailored to specific application requirements.

Here are a couple of my recommendations:
1. As an alternative to 𝗨𝗠𝗔𝗣, consider experimenting with algorithms like 𝗧𝗦𝗡𝗘 or PCA.
2. Employ LLMs capable of handling extensive contexts to process segments from various levels alongside the queries posed by the RAG application users. This allows the LLM to determine the most suitable level for each case.

Image credits - Leonardo.Ai
Acknowledging friends and mentors - VASANTH P, Vetrivel PS, Navdeep Singh, Navdeep Kaur, Mahesh Deshwal, Sanyam Bhutani, Basant Singh, Aman Chadha, Ravi Tanwar, Harpreet Sahota 🥑, F. Firat Gonen, PhD
A special shoutout to Pramodith B. - Thanks, mate, for your insightful post","[RAG application,RAPTOR,segmenting content]","Consider experimenting with algorithms like TSNE or PCA as an alternative to UMAP,Employ LLMs capable of handling extensive contexts to process segments from various levels alongside the queries posed by the RAG application users","Developing a RAG application,Stumbled upon RAPTOR, an innovative technique for segmenting content,UMAP to reduce the dimensions of segments","[RAG application,RAPTOR,segmenting content]",1,0,0
https://www.linkedin.com/posts/jobanpreet-singh-392581207_asr-whisper-speechrecognition-activity-7172803455718158336-MC-j?utm_source=share&utm_medium=member_desktop,"                🎤🔎 Unveiling the Essentials of the Advanced ASR Model: Whisper! 📄

Hello LinkedIn Community! 👋

In today's discussion, we're going to unpack the core elements of the revolutionary ASR model known as Whisper, based on a fascinating study I've been examining. Let's start with the technology that laid the groundwork: Wav2Vec2.0!

🔎 Wav2Vec2.0:
1. This model underwent unsupervised training with over 1 million hours of audio data.
2. The audio data utilized was without labels.
3. Wav2Vec2 excels in generating superior speech representations via its encoder, yet it falls short in decoding these representations without further fine-tuning for speech recognition tasks.
4. Fine-tuning of the decoder in a supervised manner is necessary for Wav2Vec2.

🌠 Whisper:
1. Whisper has been trained on a massive 680,000 hours of annotated data, which was meticulously gathered from the web. The dataset was refined by eliminating computer-generated data (ASR), ensuring the transcript and spoken language alignment, and reducing duplicates.
2. The audio files are segmented into 30-second clips.
3. The model's architecture is based on an encoder-decoder transformer. Audio samples are adjusted to a 16khz sampling rate and transformed into a log-magnitude Mel spectrogram, calculated over a 25 milliseconds window with a 10 milliseconds stride.

🔈 Log-Mel Spectrograms
Curious about log-mel spectrograms? Let's clarify:
- Spectrogram: A visual 3-D representation of sound, illustrating frequency (Y-axis) across time (X-axis).

- Log-mel Spectrogram:
➡️ It shares similarities with the spectrogram but employs a logarithmic frequency scale, known as the mel-scale, in contrast to the linear scale of the spectrogram.
➡️ On this logarithmic scale, frequencies scale exponentially. This scaling method aligns more closely with the human auditory system's perception, making changes in frequency appear more consistent than with linear scaling.

▶️The log-mel spectrogram is processed through two 1-D convolutional layers, followed by the application of the Gaussian error linear unit (GELU) activation function.
▶️Before entering the transformer encoder blocks, sinusoidal positional encoding is applied.
▶️Learned positional encoding: The model is designed to dynamically learn positional information during its training phase.

💡 Looking Ahead?
Eager for more? Keep an eye out for my upcoming post, where we'll explore the intricacies of fine-tuning Whisper. I value your thoughts and feedback, so please share them in the comments section below! 💬

Let's continue this enlightening journey together! 🚀 #ASR #Whisper #SpeechTech #AIInnovation #TechInsights","[Whisper,Wav2Vec2.0,Log-mel Spectrogram]","Whisper has been trained on 680,000 hours of annotated data,Wav2Vec2.0 underwent unsupervised training with over 1 million hours of audio data,Log-mel spectrograms use a logarithmic frequency scale aligned with human auditory perception","Whisper model trained on refined annotated data,Wav2Vec2.0 excels in speech representations but needs fine-tuning for recognition tasks,Log-mel spectrogram processed through convolutional layers and positional encoding","[Whisper,Wav2Vec2.0,Log-mel Spectrogram]",1,0,0
https://www.linkedin.com/posts/jobanpreet-singh-392581207_transliteration-a-hugging-face-space-by-activity-7117509460616445952-FsJf?utm_source=share&utm_medium=member_desktop,"                Successfully Finished My Project on Converting English to Punjabi 👉 ਇੰਗਲਿਸ਼ ਟੂ ਪੰਜਾਬੀ

Hello LinkedIn family,
I'm thrilled to announce the completion of my recent project - converting English text into Punjabi.

This project was brought to life using a sophisticated encoder-decoder framework that incorporates dual LSTM units, enabling the smooth transformation of English words into their Punjabi equivalents.

📊 Evaluating Quality:
To determine the effectiveness of this conversion tool, I utilized the Word Accuracy metric. This approach assesses the proportion of words correctly converted against the total words tested, offering a transparent evaluation of the tool's precision and dependability.

📈 Formula for Word Accuracy:
Accuracy = (Words Correctly Converted / Total Words Tested) * 100
Where:
Words Correctly Converted (C) = Number of words accurately transformed
Total Words Tested (N) = Total words evaluated

🙏 Appreciation:
My sincere thanks go to AI4Bhārat for their crucial contribution of a dataset containing English words and their Punjabi translations, which was essential for the success of this endeavor.

🚀 Interface Development:
For an intuitive user experience, the project's interface was developed using Gradio.

🌐 Project Availability:
This tool is now available for everyone to use via Huggingface Spaces. Feel free to try it out in real-time by clicking on the link below:
https://lnkd.in/dJngxES9
#Transliteration #Huggingface #AI4BHARAT","[English to Punjabi,encoder-decoder framework, LSTM units]","Successfully Finished My Project on Converting English to Punjabi,Utilized Word Accuracy metric for evaluation,AI4Bhārat dataset contribution","Sophisticated encoder-decoder framework with dual LSTM units,Word Accuracy metric for evaluation of conversion tool,AI4Bhārat dataset contribution for project success","[English to Punjabi,encoder-decoder framework,Word Accuracy]",0.5,0.112032426936064,0.030446
https://www.linkedin.com/posts/mohamedsalama1_ai-reinforcementlearning-innovation-activity-7178792892411375616-bznm?utm_source=share&utm_medium=member_desktop,"                Introducing ReFT: Revolutionizing LLM Fine-Tuning 🧠 ♻

A team of researchers has introduced a groundbreaking method called Reinforced Fine-Tuning (ReFT) to boost the learning efficiency of Large Language Models (LLMs). This innovative approach diverges from conventional techniques by employing reinforcement learning to assess different reasoning paths, achieving remarkable progress without the need for extra training data. 🔍💡

ReFT enhances LLMs by initially applying a warm-up phase using SFT, then transitioning to real-time learning. During this phase, it experiments with various reasoning paths and reinforces those that lead to accurate outcomes. This method has shown to outperform traditional SFT alone, with the potential for even greater enhancements through future inference-time strategies.

What sets ReFT apart is its ability to improve learning efficiency without expanding the dataset. It focuses on intelligent learning strategies rather than accumulating more data. Despite potential concerns regarding reward manipulation, the initial outcomes are solid and offer great promise!

Discover more about this game-changing technique here: https://lnkd.in/d254BwaF
#AI #ReinforcementLearning #Innovation #LLMs","[ReFT,LLM Fine-Tuning,Large Language Models]","ReFT enhances LLMs by applying a warm-up phase using SFT and transitioning to real-time learning,ReFT focuses on intelligent learning strategies rather than accumulating more data,ReFT shows potential for even greater enhancements through future inference-time strategies","ReFT is a groundbreaking method to boost the learning efficiency of Large Language Models,ReFT employs reinforcement learning to assess different reasoning paths,ReFT outperforms traditional SFT alone","[ReFT,LLM Fine-Tuning,Large Language Models]",1,0,0
https://www.linkedin.com/posts/bavalpreet-singh_200205202v1pdf-activity-7166186517621628928-1JLm?utm_source=share&utm_medium=member_desktop,"#Google has recently unveiled #Gemma, a groundbreaking series of ultra-light models. These models are developed with the advanced technology that also drives the Gemini series.

Discover the unique features of Gemma:

Model Variants: Gemma is available in two models: 2B and 7B. Each model is offered in an instruction-based version and a basic version that excludes complex multimedia elements.

Corporate Compatibility: Gemma models are fully equipped for business applications, enabling organizations to leverage them for various purposes.

Advanced Processing: With an impressive 8192 token context window, Gemma is capable of processing extensive amounts of data simultaneously.

Exceptional Performance: In comparison to its counterparts like Mistral AI 7B and LLaMa 2, the 7B model of Gemma excels in Human Eval and MMLU evaluations, achieving a score of 64.56 on MMLU.

Gemma's architecture introduces several significant enhancements over traditional transformer models, including:

1. Improved Attention and Positional Encoding: The use of Multi-Query Attention and RoPE enhances attention mechanisms and positional encoding, promoting more effective learning.

2. Adaptive Model Responses: The introduction of GeGLU (link to paper = https://lnkd.in/eQ58JGGS) Activations instead of the conventional ReLU allows for more adaptable responses from the model.

3. Enhanced Layer Stability: By applying RMSNorm, Gemma achieves greater stability across its layers through advanced normalization techniques.

The best aspect? Gemma is accessible on platforms like #huggingface, Kaggle, and #vertexai, making it easy for anyone to acquire and start utilizing it!

https://lnkd.in/evTMrmyV
#googlegemma, #aimodels, #gemmamodels, #cuttingedgeai, #techinnovation, #machinelearning, #nlp, #llm, #aiforbusiness, #transformativetech, #modelperformance, #nextgenmodels, #techadvancements, #googleresearch, #stateoftheart, #modeldevelopment, #mlcommunity, #aiapplications, #industrysolutions, #digitaltransformation, #modeldeployment","[Gemma,models,technology]","Gemma is available in two models: 2B and 7B,Gemma models are fully equipped for business applications,Gemma is capable of processing extensive amounts of data simultaneously","Model Variants: Gemma is available in two models: 2B and 7B,Advanced Processing: With an impressive 8192 token context window, Gemma is capable of processing extensive amounts of data simultaneously,Exceptional Performance: In comparison to its counterparts like Mistral AI 7B and LLaMa 2, the 7B model of Gemma excels in Human Eval and MMLU evaluations, achieving a score of 64.56 on MMLU","[Gemma,features of Gemma,Gemma’s architecture]",0.2,0.398185797085841,0.110311
https://www.linkedin.com/posts/vasanthengineer4949_elonmusk-grokai-xai-activity-7176584570530611200-ALUP?utm_source=share&utm_medium=member_desktop,"Grok - 1, the newest MOE model boasting a 31B parameter size, has been unveiled by X and Elon Musk, featuring an impressive 86B active parameters. This model incorporates a blend of traditional and innovative modifications in its design. I've delved into these aspects and included the code in my latest video.

Link: [https://lnkd.in/dkJv2yt3](https://lnkd.in/dkJv2yt3)

Highlights:
1. A 314B parameter MOE model with 86B operational parameters
2. Developed using the JAX framework
3. This version is the initial pre-trained model, not yet fine-tuned
4. Available under the Apache 2.0 License
5. It is uncensored
6. Model weights can be found on Hugging Face

Despite its 314B parameters, its performance in benchmarks doesn't necessarily match up, a topic I've thoroughly explored in the video.

This image provides essential details about the model's architecture.

Mentioning a few from my network: Harpreet Sahota 🥑Vetrivel PS Sahibpreet Singh AbdulMajedRaja RS Krish Naik Ashish Patel 🇮🇳 Arpit Singh Anshul Saxena, PhD SUNNY BHAVEEN CHANDRA Manish Sharma 📊 Leandro von Werra Clem Delangue 🤗Thomas Wolf Lewis Tunstall Maxime Labonne Luis Serrano Omar Sanseviero Aladdin Persson Yannic Kilcher Umar Jamil

#elonmusk #grokai #xai","[Grok - 1,MOE model,31B parameter size]","A 314B parameter MOE model with 86B operational parameters,Developed using the JAX framework,This version is the initial pre-trained model, not yet fine-tuned","A 314B parameter MOE model with 86B operational parameters,Developed using the JAX framework,This version is the initial pre-trained model, not yet fine-tuned","[Grok - 1,MOE model,31B parameter size]",1,0,0
https://www.linkedin.com/posts/vasanthengineer4949_wanna-know-how-to-connect-your-llms-to-csv-activity-7175860156151332864-13Me?utm_source=share&utm_medium=member_desktop,"Curious about linking your Large Language Models (LLMs) with a CSV as a database?

Explore: Mixtral Database Administrator: Engage with Your Database

For a detailed guide, visit: [Link Removed]

In a recent tutorial, I demonstrated how to utilize the Groq Inference Engine to operate Mixtral by Mistral AI through a Streamlit Interface. To transform a CSV file into an SQL database, I employed pandasql, and for prompt formatting, I used LangChain.

Procedure:
1. Convert a CSV file into an SQL schema.
2. Present a user query alongside the SQL Schema to Mistral.
3. Mistral crafts an SQL query.
4. Execute the generated query on the database within a try-except block.
5. Embed everything into a user interface for an enhanced experience.

The code is showcased in the accompanying image. For an in-depth exploration and insights into my journey of fine-tuning LLMs for SQL query generation, visit the link above.

Mentioning a few of my connections: Harrison Chase, Vetrivel PS, Sahibpreet Singh, Ashish Patel 🇮🇳, Arpit Singh, Anshul Saxena, PhD, SUNNY BHAVEEN CHANDRA, Manish Sharma 📊, Krish Naik.","[Large Language Models,LLMs,CSV]","Convert a CSV file into an SQL schema,Present a user query alongside the SQL Schema to Mistral,Mistral crafts an SQL query","Utilizing the Groq Inference Engine to operate Mixtral by Mistral AI through a Streamlit Interface,Transforming a CSV file into an SQL database using pandasql and LangChain,Showcasing code in the accompanying image","[Large Language Models,LLMs,CSV]",1,0,0
https://www.linkedin.com/posts/vasanthengineer4949_devin-going-take-your-jobs-devin-working-activity-7175500962533396480-FMw5?utm_source=share&utm_medium=member_desktop,"Following the unveiling of the groundbreaking Devin AI by Cognition, there's been a buzz about whether it's a threat to human jobs.

However, I believe it's crucial first to grasp its mechanism. I've delved into the intricacies of Devin AI in this video, providing a comprehensive overview:

Is Devin AI a Job Threat? | Unpacking Devin's Functionality

Link: [Video Link]

In the video, I thoroughly discuss Devin, addressing all potential queries you might have, such as:

1. The essence of Devin AI
2. The reasons behind Devin's efficiency
3. The potential of Devin AI
4. The possibility of Devin AI taking over human roles
5. A detailed breakdown of how Devin operates

Eager to hear your thoughts as well...

Mentioning a few of my connections: Sahibpreet Singh, Vetrivel PS, Ashish Patel 🇮🇳, SUNNY BHAVEEN CHANDRA, Krish Naik, AbdulMajedRaja RS, Manish Sharma 📊","[Devin AI,Cognition,job threat]","Devin AI unveiling by Cognition,Understanding Devin AI mechanism,Comprehensive overview of Devin AI","Devin AI unveiling by Cognition,Comprehensive overview of Devin AI in video,Discussion on Devin's essence, efficiency, potential, and operation","[Devin AI,Cognition,job threat]",1,0,0
,,,,,,Average= 0.79,Average= 0.096,Average= 0.026
